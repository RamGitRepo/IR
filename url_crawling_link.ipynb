{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "CRAWL_PROFILE = True\n",
    "CRAWL_ARTICLE = True\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "home_url='https://pureportal.coventry.ac.uk/en/organisations/coventry-university/persons/'\n",
    "default_url='https://pureportal.coventry.ac.uk'\n",
    "articles_inputFile = \"C:/Users/ramna/DataScience/IR/7071\\Scraped_Articles_Details.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue= [home_url]\n",
    "already_visited=[]\n",
    "total_entries= 0\n",
    "last_page_entries =0\n",
    "profile_id = 1\n",
    "document_id =1\n",
    "data_table = {}\n",
    "article_table = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching home page data details for Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input : page-class:`Response <Response>` object from GET request. \n",
    "# Update profile information in the global data_table\n",
    "def home_page_details(page):\n",
    "    global default_url\n",
    "    global total_entries\n",
    "    global queue\n",
    "    global last_page_entries\n",
    "    global data_table\n",
    "    global profile_id\n",
    "\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    users = soup.find_all('div',attrs={'class':'result-container'})\n",
    "    no_of_entries_in_page = 0\n",
    "    for user in users:\n",
    "        profile_name=user.find('span').text\n",
    "        link = user.find('a')['href']\n",
    "        no_of_entries_in_page +=1\n",
    "\n",
    "        data_table[profile_id] = [profile_name,link]\n",
    "        \n",
    "        profile_id =profile_id+1\n",
    "\n",
    "    if last_page_entries == 0:\n",
    "        last_page_entries = no_of_entries_in_page\n",
    "\n",
    "    if no_of_entries_in_page <last_page_entries : # We have reached to last page\n",
    "        total_entries=total_entries+no_of_entries_in_page\n",
    "    else:    \n",
    "        total_entries=total_entries+no_of_entries_in_page\n",
    "        next_page=soup.find('a',attrs={'class':'nextLink'})['href']\n",
    "        next_page_link=default_url+next_page\n",
    "        queue.append(next_page_link)\n",
    "        last_page_entries = no_of_entries_in_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching next page data details for Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input - Profile link\n",
    "# output - update global article_table with all the details related with the profile \n",
    "def article_details(profile_url):\n",
    "    global article_table\n",
    "    global document_id\n",
    "    start = 0\n",
    "    page_size=20    \n",
    "    #access url of profile \n",
    "    url = \"/publications/\"\n",
    "    #print(\"Url ::\", profile_url+url)\n",
    "    page =requests.get(profile_url+url)   \n",
    "    \n",
    "    if page.status_code != 200:\n",
    "        print(\"Failed to access url..[ERROR_CODE]:\", page.status_code)\n",
    "        print(\"Profile Url : \",profile_url)\n",
    "        raise Exception(\"Error loading page..\")\n",
    "\n",
    "    else:\n",
    "        #read the page HTML content\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        #find  all the articles on single page\n",
    "        name = soup.find('h1').text\n",
    "        titles = soup.find_all('h3', attrs={'class': 'title'})\n",
    "#       next_page=soup.find('div',attrs={'class':'view-all align-center'})\n",
    "        while len(titles) > 0: # NOT NULL\n",
    "            no_of_citations = soup.find_all('div',attrs={'class':'metric scopus-citations'})\n",
    "            for title, citation in zip(titles,no_of_citations):\n",
    "                \n",
    "                profile_name = name\n",
    "                title_name = title.find('a',attrs={'class': 'link'}).text\n",
    "                co_authors = soup.find('a',attrs={'class': 'link person'}).text\n",
    "                title_link = title.find('a')['href']\n",
    "                no_Of_Citations = citation.find('a').text\n",
    "                \n",
    "                article_table[document_id] = [profile_name,title_name,co_authors,title_link,no_Of_Citations]\n",
    "\n",
    "                document_id=document_id+1 # increment the document id for next item\n",
    "            \n",
    "#           nxt_pge = next_page.find('a', attrs={'class': 'portal_link btn-primary btn-large'})['href']\n",
    "#           new_page_link = requests.get(default_url+nxt_pge)\n",
    "#           soup = BeautifulSoup(new_page_link.content, 'html.parser')\n",
    "            titles = soup.find_all('h3', attrs={'class': 'title1'})            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "page = requests.get(home_url)\n",
    "print(page.status_code)# Test code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1 :: Fetching all the pages for profile links into CSV File\n",
      "\n",
      "Home URL :  https://pureportal.coventry.ac.uk/en/organisations/coventry-university/persons/\n",
      "\n",
      "Saving all the crawling pages for profiles links into CSV File.\n",
      "\n",
      "Saved Successfully\n",
      "\n",
      "Step 2 :: Fetching all the Articles From Profile URL into CSV File\n",
      "\n",
      "Profile URL :  https://pureportal.coventry.ac.uk/en/persons/mohammadreza-abbassi-monjezi\n",
      "\n",
      "Total Iteration Page:  2185\n",
      "\n",
      "Saving all the Articles details into CSV File\n",
      "\n",
      "Saved Successfully\n"
     ]
    }
   ],
   "source": [
    "#hard stop after 500 loops to avoid any server overload due to infinite crawling\n",
    "stop = 100 # Use only for top level of scraping \n",
    "start_time = time.time() # measure time taken to scrape data \n",
    "failure=False\n",
    "\n",
    "\n",
    "\n",
    "if CRAWL_PROFILE:\n",
    "    #1.1 Crawl through all the pages and extract profile links into a list\n",
    "    print(\"\\nStep 1 :: Fetching all the pages for profile links into CSV File\")\n",
    "    print(\"\\nHome URL : \",home_url)\n",
    "    global data_table\n",
    "    col_names_profile=['Profile_Name','Profile_Link']\n",
    "\n",
    "\n",
    "    while len(queue)!=0 and stop >0:\n",
    "        try:\n",
    "            random_time=random.randint(0,1) # genrate random time wait in sec\n",
    "            #retrieve HTML content of the page\n",
    "        #     print(\"url : \",queue[0])\n",
    "            page =requests.get(queue[0])\n",
    "\n",
    "            #extract all the requried info from the page\n",
    "            if page.status_code==200:\n",
    "                home_page_details(page)\n",
    "            else:\n",
    "                print(\"Failed to load page [ERROR_CODE]: \",page.status_code)\n",
    "                print(\"Page Url : \",queue[0])\n",
    "\n",
    "            #Try next link -> pop out the first page from main queue::FIFO and add into already visited url list\n",
    "            #print(len(queue))\n",
    "            if len(queue)>0 :\n",
    "                already_visited.append(queue.pop(0))\n",
    "            stop = stop-1\n",
    "            time.sleep(random_time)  # add delay of 1s before visiting next page\n",
    "        except:\n",
    "            print(\"Inside Step 1 fetching....something went wrong..Exiting\\n\")\n",
    "            failure=True\n",
    "            break\n",
    "\n",
    "    stop_time = time.time()        \n",
    "    if failure==False:\n",
    "        time_taken = stop_time-start_time\n",
    "        \n",
    "        print(\"\\nSaving all the crawling pages for profiles links into CSV File.\")\n",
    "        #Write profile data into file\n",
    "        \n",
    "\n",
    "        data_frame=pd.DataFrame.from_dict(data_table,orient ='index',columns=col_names_profile)\n",
    "        data_frame.index.rename('Profile_Id', inplace=True)\n",
    "    \n",
    "        file_exists = os.path.isfile(\"Scraped_Profile_Details.csv\")\n",
    "        if file_exists: #backup the file\n",
    "            shutil.copy(\"Scraped_Profile_Details.csv\", \"Scraped_Profile_Details_Last.csv\")  \n",
    "        \n",
    "        data_frame.to_csv(\"Scraped_Profile_Details.csv\")\n",
    "        \n",
    "        print(\"\\nSaved Successfully\")         \n",
    "    else:\n",
    "        time_taken = stop_time-start_time\n",
    "        \n",
    "        print(\"\\nSaving all the crawling pages for profiles links into CSV File.\")\n",
    "        #Write profile data into file\n",
    "        #save_data_profile()  \n",
    "\n",
    "        data_frame=pd.DataFrame.from_dict(data_table,orient ='index',columns=col_names_profile)\n",
    "        data_frame.index.rename('Profile_Id', inplace=True)\n",
    "    \n",
    "        file_exists = os.path.isfile(\"Scraped_Profile_Details.csv\")\n",
    "        if file_exists: #backup the file\n",
    "            shutil.copy(\"Scraped_Profile_Details.csv\", \"Scraped_Profile_Details_Last.csv\")  \n",
    "        \n",
    "        data_frame.to_csv(\"Scraped_Profile_Details.csv\") \n",
    "        \n",
    "        print(\"Step 1: fetching of profiles failed\")\n",
    "\n",
    "if CRAWL_ARTICLE:    \n",
    "    start_time = time.time() # measure time taken to scrape data \n",
    "    #1.2 Crawl through all the profiles link now to extract articles for each users\n",
    "    profile_df =pd.read_csv(\"Scraped_Profile_Details.csv\",header='infer')\n",
    "    profile_queue = profile_df['Profile_Link'].values # assign the profile_queue with list of all the pofile link ::FIFO\n",
    "    head_profile_url = profile_queue[0]\n",
    "    stop = len(profile_queue)\n",
    "    global article_table\n",
    "    col_names_article=['Profile_Name','Title','Co_Authors','Title_Link','No_Of_Citations']\n",
    "\n",
    "    print(\"\\nStep 2 :: Fetching all the Articles From Profile URL into CSV File\")\n",
    "    print(\"\\nFirst Page of Profile URL : \",head_profile_url)\n",
    "    print(\"\\nTotal Page Fetched: \",len(profile_queue))\n",
    "\n",
    "    while len(profile_queue) != 0 and stop >0:\n",
    "        try:\n",
    "            stop = stop-1\n",
    "            #random_time=random.randint(0,1) # genrate random time wait in sec\n",
    "\n",
    "               # Crawl to individual user profile and extract their published articles details\n",
    "            article_details(head_profile_url)\n",
    "            #print(\"Processsing done , pending \", len(profile_queue)-1)\n",
    "                #pop out the first link from the queue and assign second link as first item\n",
    "            if len(profile_queue)!=1:\n",
    "                profile_queue = profile_queue[1:]\n",
    "                #assign head of queue to head_profile_url for next crawl\n",
    "                head_profile_url = profile_queue[0]\n",
    "            else:\n",
    "                profile_queue = [] # All the links are crawled\n",
    "                \n",
    "            time.sleep(random_time)  # add delay of 1s before visiting next page\n",
    "        except:\n",
    "            failure = True\n",
    "            break\n",
    "\n",
    "    stop_time = time.time()\n",
    "    if failure == False:\n",
    "        time_taken = stop_time-start_time\n",
    "\n",
    "        print(\"Nothing to process. Queue Len : \",len(queue))\n",
    "        print(\"No.Of Entries extracted : \", total_entries)\n",
    "        print(\"\\nCompleted before hard stop, Iterations still left before hard stop(max 500 iterations) : \",stop)\n",
    "\n",
    "        #Write published article data into file\n",
    "        print(\"\\nSaving all the Articles details into CSV File\")    \n",
    "\n",
    "        data_frame=pd.DataFrame.from_dict(article_table,orient='index',columns=col_names_article)\n",
    "        data_frame.index.rename('Document_Id', inplace=True)\n",
    "    \n",
    "        file_exists = os.path.isfile(\"Scraped_Articles_Details.csv\")\n",
    "        if file_exists: #backup the file\n",
    "            shutil.copy(\"Scraped_Articles_Details.csv\", \"Scraped_Articles_Details_Last.csv\") \n",
    "        \n",
    "        data_frame.to_csv(\"Scraped_Articles_Details.csv\")\n",
    "        \n",
    "        print(\"\\nSaved Successfully.\")  \n",
    "    else:\n",
    "        time_taken = stop_time-start_time\n",
    "        \n",
    "        \n",
    "        print(\"\\nSaving all the Articles details into CSV File\")  \n",
    "        data_frame=pd.DataFrame.from_dict(article_table,orient='index',columns=col_names_article)\n",
    "        data_frame.index.rename('Document_Id', inplace=True)\n",
    "    \n",
    "        file_exists = os.path.isfile(\"Scraped_Articles_Details.csv\")\n",
    "        if file_exists: #backup the file\n",
    "            shutil.copy(\"Scraped_Articles_Details.csv\", \"Scraped_Articles_Details_Last.csv\") \n",
    "        \n",
    "        data_frame.to_csv(\"Scraped_Articles_Details.csv\")\n",
    "        \n",
    "        print(\"\\nSaved Successfully\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

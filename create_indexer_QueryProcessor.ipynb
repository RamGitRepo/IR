{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "BUILD_INDEX = True\n",
    "BUILD_INDEX_TEST = False\n",
    "TEST_QUERY = False\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "virgin_index_creation = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###function to create inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUILD_INDEX: \n",
    "\n",
    "    def inverted_indexCreation(inputFile):\n",
    "#         stop =5 # for debugging\n",
    "        new_word = True\n",
    "        wordsAdded = {}\n",
    "\n",
    "        data_type = {'Document_Id' : 'int',\n",
    "                     'Profile_Name':'str',\n",
    "                     'Title':'str',\n",
    "                     'Co_Authors':'str',\n",
    "                     'Title_Link': 'str',\n",
    "                     'No_Of_Citations':'str'\n",
    "                    }\n",
    "\n",
    "        data = pd.read_csv(inputFile,header='infer',dtype=data_type)\n",
    "\n",
    "        exlcude_puncuation = [';', '.', ',', '!', ':', '-','_']\n",
    "        #set stopwords for english language\n",
    "        sw = stopwords.words('english')\n",
    "\n",
    "        ps=PorterStemmer()\n",
    "        filtered_docs = []\n",
    "        docs = data['Profile_Name']+\" \"+data['Title']+\" \"+data['Co_Authors'].astype('str')# combine name title year for indexing\n",
    "#         print(docs)\n",
    "        for title,doc_id in zip(docs,data.Document_Id):\n",
    "            tokens = word_tokenize(str(title))\n",
    "            temp = ''\n",
    "            word_count =1\n",
    "#             print(\"\\n\",tokens,\"WC\",word_count)\n",
    "#             break\n",
    "            for word in tokens:\n",
    "                if word not in sw and word not in exlcude_puncuation and len(word)>2:\n",
    "                    word_stem =ps.stem(word) # stem the word\n",
    "                    if word_stem not in wordsAdded.keys(): # Create a new entry in dictionary for new words : terms and posting\n",
    "                        wordsAdded[word_stem]=[]\n",
    "                        word_count=1\n",
    "                        new_word =True\n",
    "                    else:\n",
    "                        new_word=False\n",
    "\n",
    "\n",
    "                    if new_word==True: # Adding word and count first time into dicttionary\n",
    "                        #print(\"inside ..\",word_stem,doc_id,word_count,\"\\n\")\n",
    "                        wordsAdded[word_stem].append((doc_id,word_count))\n",
    "                        word_count +=1 # increment the existing word count\n",
    "                    else: # only update the doc_id and count for word                        \n",
    "                        #find position of tuple to update count of word in the index dictionary\n",
    "                        val_length = len(wordsAdded[word_stem]) # find how many entries are already made for word\n",
    "#                         print(\"\\nAlready existing dict : \", word_stem, wordsAdded[word_stem])\n",
    "                        for count in range(val_length):\n",
    "                            if wordsAdded[word_stem][count][0] == doc_id : # find the location of tuple with matching doc id \n",
    "                                location=True\n",
    "                                position=count\n",
    "                                break # found the location of tuple for matching doc_id so break the loop\n",
    "\n",
    "                        if location == True:\n",
    "                            existing_word_count = wordsAdded[word_stem][position][1]\n",
    "                            wordsAdded[word_stem][position] = (doc_id, existing_word_count+1)\n",
    "                            #reset flags\n",
    "                            location = False\n",
    "                            position=-1\n",
    "                        else:  # its a new doc_id entry for existing word\n",
    "                            # add a new doc_id and count info\n",
    "                            wordsAdded[word_stem].append((doc_id, 1))\n",
    "\n",
    "#                         print(\"\\nAlready existing dict (After update): \", word_stem, wordsAdded[word_stem])\n",
    "                        #print(\"inside ..\",word_stem,doc_id+1,word_count,\"\\n\")\n",
    "                    temp = temp + word_stem + \" \"\n",
    "\n",
    "            filtered_docs.append(temp)\n",
    "#             stop=stop-1\n",
    "#             if stop <0: break\n",
    "        data['Filtered_title'] =  filtered_docs \n",
    "\n",
    "        return wordsAdded,data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update already created index\n",
    "# return the filtered new data and update the global inverted index\n",
    "#input dataframe with channged rows\n",
    "def update_index(data):\n",
    "    global inv_index\n",
    "    new_word = True\n",
    "    \n",
    "    exlcude_puncuation = [';', '.', ',', '!', ':', '-','_']\n",
    "    #set stopwords for english language\n",
    "    sw = stopwords.words('english')\n",
    "    ps=PorterStemmer()\n",
    "    filtered_docs = []    \n",
    "    \n",
    "    docs = data['Profile_Name']+\" \"+data['Title']+\" \"+data['Co_Authors'].astype('str')# combine name title year for indexing \n",
    "    \n",
    "    if virgin_index_creation == False:\n",
    "        if len(inv_index):\n",
    "            for title,doc_id in zip(docs,data.Document_Id):\n",
    "                tokens = word_tokenize(str(title))\n",
    "                temp = ''\n",
    "                word_count =1\n",
    "    #             print(\"\\n\",tokens,\"WC\",word_count)\n",
    "    #             break\n",
    "                for word in tokens:\n",
    "                    if word not in sw and word not in exlcude_puncuation and len(word)>2:\n",
    "                        word_stem =ps.stem(word) # stem the word\n",
    "                        if word_stem not in inv_index.keys(): # Create a new entry in dictionary for new words : terms and posting\n",
    "                            inv_index[word_stem]=[]\n",
    "                            word_count=1\n",
    "                            new_word =True\n",
    "                        else:\n",
    "                            new_word=False\n",
    "\n",
    "\n",
    "                        if new_word==True: # Adding word and count first time into dicttionary\n",
    "                            #print(\"inside ..\",word_stem,doc_id,word_count,\"\\n\")\n",
    "                            inv_index[word_stem].append((doc_id,word_count))\n",
    "                            word_count +=1 # increment the existing word count\n",
    "                        else: # only update the doc_id and count for word                        \n",
    "                            #find position of tuple to update count of word in the index dictionary\n",
    "                            val_length = len(inv_index[word_stem]) # find how many entries are already made for word\n",
    "    #                         print(\"\\nAlready existing dict : \", word_stem, wordsAdded[word_stem])\n",
    "                            for count in range(val_length):\n",
    "                                if inv_index[word_stem][count][0] == doc_id : # find the location of tuple with matching doc id \n",
    "                                    location=True\n",
    "                                    position=count\n",
    "                                    break # found the location of tuple for matching doc_id so break the loop\n",
    "\n",
    "                            if location == True:\n",
    "                                existing_word_count = inv_index[word_stem][position][1]\n",
    "                                inv_index[word_stem][position] = (doc_id, existing_word_count+1)\n",
    "                                #reset flags\n",
    "                                location = False\n",
    "                                position=-1\n",
    "                            else:  # its a new doc_id entry for existing word\n",
    "                                # add a new doc_id and count info\n",
    "                                inv_index[word_stem].append((doc_id, 1))\n",
    "\n",
    "    #                         print(\"\\nAlready existing dict (After update): \", word_stem, wordsAdded[word_stem])\n",
    "                            #print(\"inside ..\",word_stem,doc_id+1,word_count,\"\\n\")\n",
    "                        temp = temp + word_stem + \" \"\n",
    "\n",
    "                filtered_docs.append(temp)\n",
    "    #             stop=stop-1\n",
    "    #             if stop <0: break\n",
    "                data['Filtered_title'] =  filtered_docs \n",
    "            print(\"Update Completed Successfully...\")\n",
    "            return data           \n",
    "         \n",
    "        else:\n",
    "            print(\"Inverted index is empty..please first create the index\")            \n",
    "    else:\n",
    "        print(\"Please first create inverted index..\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time taken to build index : 2.4963 sec\n",
      "Index Size 5082\n"
     ]
    }
   ],
   "source": [
    "if BUILD_INDEX:\n",
    "    if virgin_index_creation:  # create whole index for the first time\n",
    "        articles_inputFile = \"Scraped_Articles_Details.csv\"\n",
    "        start_time = time.time() # measure time taken to build index \n",
    "\n",
    "        display_data = pd.read_csv(articles_inputFile,header='infer')\n",
    "        display_data.fillna(value=0,inplace=True)\n",
    "        inv_index, data =inverted_indexCreation(articles_inputFile)\n",
    "\n",
    "        stop_time = time.time() # stop the timer\n",
    "        time_taken = stop_time-start_time # time taken to build index\n",
    "        print(\"\\nTime taken to build index :\",str(np.round(time_taken,4))+\" sec\") \n",
    "        print(\"Index Size\", len(inv_index))\n",
    "        virgin_index_creation = False\n",
    "        boot_file = open(\"robots.txt\",'w')\n",
    "        boot_file.write(\"0\")\n",
    "        boot_file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index update main function\n",
    "def index_update():\n",
    "    read_file = open(\"robots.txt\",'r')\n",
    "    flag = read_file.read()\n",
    "    if int(flag) ==0:\n",
    "        print(\"Index update only\")\n",
    "\n",
    "        # Compare the data of last crawl with latest crawl data\n",
    "        last_file = \"Scraped_Articles_Details_Last.csv\"\n",
    "        latest_file = \"Scraped_Articles_Details.csv\"\n",
    "\n",
    "        last_data = pd.read_csv(last_file,header='infer')\n",
    "        latest_data = pd.read_csv(latest_file, header='infer')\n",
    "\n",
    "        #check number of rows in each of the data\n",
    "        size_last = len(last_data)\n",
    "        size_latest= len(latest_data)\n",
    "\n",
    "        print(\"Old File size : \",size_last)\n",
    "        print(\"New File size : \",size_latest)\n",
    "        \n",
    "        if size_last==size_latest:\n",
    "            difference = latest_data[last_data.ne(latest_data).any(axis=1)]\n",
    "\n",
    "        elif size_last > size_latest:\n",
    "            difference = last_data[latest_data.ne(last_data).any(axis=1)]\n",
    "        else:\n",
    "            difference = latest_data[last_data.ne(latest_data).any(axis=1)]\n",
    "        \n",
    "        filtered_data = update_index(difference)\n",
    "        return filtered_data\n",
    "    else:\n",
    "        print(\"No update performed as index flag is not set to 0\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test index\n",
    "if BUILD_INDEX_TEST:\n",
    "\n",
    "    term ='machine'\n",
    "    start_time = time.time() # measure time taken to search data     \n",
    "    ps=PorterStemmer()\n",
    "\n",
    "    test_word =ps.stem(term)\n",
    "    \n",
    "    if test_word in inv_index.keys():\n",
    "        posting =inv_index[test_word]\n",
    "\n",
    "        stop_time = time.time() # stop the timer\n",
    "        time_taken = stop_time-start_time\n",
    "        print(\"\\nTime taken to search term in Index :\",str(np.round(time_taken,4))+\" sec\") \n",
    "        #test index\n",
    "    #     print(posting)\n",
    "        if len(posting)!= 0:\n",
    "            print(\"Searched term: \",term)\n",
    "            print(\"Total matching documents (\",len(posting),\") are found::\")\n",
    "            for position,_ in zip(range(len(posting)),range(3)): \n",
    "                doc_id = posting[position][0]\n",
    "                print(\"\\nDoc_Id: \",doc_id,end=' : ')\n",
    "                print(\"\\nDoc_Link: \",list(display_data[display_data['Document_Id'] == doc_id]['Title_Link']))\n",
    "                print(\"Title : \", list(display_data[display_data['Document_Id'] == doc_id]['Title']))\n",
    "                print(\"Profile Name : \", list(display_data[display_data['Document_Id'] == doc_id]['Profile_Name']))\n",
    "                print(\"Authors Name : \", list(display_data[display_data['Document_Id'] == doc_id]['Co_Authors']))\n",
    "                print(\"No.Of Citations : \", list(display_data[display_data['Document_Id'] == doc_id]['No_Of_Citations']))\n",
    "                \n",
    "    else:\n",
    "        print(\"Searched Term: \",test_word)\n",
    "        print(\"No Match found in the index..\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Processor engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate ranking\n",
    "def query_doc_score(term):\n",
    "    document_id = []\n",
    "    document_score = []\n",
    "    df=pd.DataFrame(columns=['Document_Id','Document_Score'],index=None)\n",
    "    if term in inv_index.keys():\n",
    "        posting  = inv_index[term]\n",
    "        for doc_id in posting:\n",
    "            tf = calc_frequency(term,doc_id[0])\n",
    "            idf = calc_idf_frequency(term)\n",
    "            score_idf = calc_tf_idf_frequency(tf,idf)\n",
    "            document_id.append(doc_id[0])\n",
    "            document_score.append(score_idf)             \n",
    "\n",
    "        df['Document_Id'],df['Document_Score']=document_id,document_score\n",
    "        return df\n",
    "    else:\n",
    "        return df # empty data frame\n",
    "    \n",
    "    \n",
    "#calculate ter frequency \n",
    "#TF(t,d) =  Log10 (1 + frequency of t in d)\n",
    "def calc_frequency(term,document):\n",
    "    global inv_index\n",
    "    if term in inv_index.keys():\n",
    "        for i in range(len(inv_index[term])):\n",
    "            if inv_index[term][i][0] == document:\n",
    "#                 print(term, inv_index[term][i][1])\n",
    "                return ( round(math.log10(1+inv_index[term][i][1]),4))\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "#calculate inverted document frequency ( idf)\n",
    "#IDF(t) = log10 (N/(df(t))\n",
    "def calc_idf_frequency(term):\n",
    "    global inv_index\n",
    "    if term in inv_index.keys():\n",
    "        total_no_of_documents = max(inv_index.values())[-1][0] # find total no.of docuements\n",
    "\n",
    "        document_freq = word_frequency_count(inv_index[term])[0] # find document containing terms\n",
    "#         print(total_no_of_documents, document_freq)\n",
    "        return  math.log(total_no_of_documents/document_freq)\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "#calcuate score tf.tdf \n",
    "def calc_tf_idf_frequency(tf, idf):\n",
    "#     print(tf, idf,tf*idf)\n",
    "    return round(tf*idf,4)\n",
    "\n",
    "#returns word frequency and and no. of documents \n",
    "def word_frequency_count(posting):\n",
    "    word_freq =0\n",
    "    for inv_index in range(len(posting)):\n",
    "        word_freq = word_freq+posting[inv_index][1]\n",
    "        \n",
    "    return(len(posting),word_freq)\n",
    "\n",
    "\n",
    "#update document score for all the terms in a query\n",
    "def update_document_score_query(term_doc_score):\n",
    "    global document_score_query \n",
    "    \n",
    "    for doc_id,doc_score in zip(term_doc_score['Document_Id'],term_doc_score['Document_Score']) :\n",
    "#         print(\"Doc_id: \",doc_id,doc_score)  \n",
    "        if doc_id not in document_score_query.keys():\n",
    "            document_score_query[doc_id] = doc_score\n",
    "        else:\n",
    "#             print(\"Old score: \",  document_score_query[doc_id])\n",
    "            document_score_query[doc_id] = document_score_query[doc_id]+doc_score\n",
    "#             print(\"New score: \",  document_score_query[doc_id])\n",
    " \n",
    "# global query score dictionary\n",
    "document_score_query = {\"doc_id\":[],\"doc_score\":[]}\n",
    "\n",
    "def query_processor_engine(query):\n",
    "    # initialize STEM and stopwords\n",
    "    global inv_index\n",
    "    not_found = []\n",
    "    processed = False\n",
    "    ps=PorterStemmer()\n",
    "    exlcude_puncuation = [';', '.', ',', '!', ':', '-','_']\n",
    "    #set stopwords for english language\n",
    "    stop_words = stopwords.words('english')\n",
    "    \n",
    "    # tokenize\n",
    "    token = word_tokenize(query)\n",
    "    term_freq_table = {}\n",
    "    # remove sropwords\n",
    "    for word in token:\n",
    "        if word not in stop_words and word not in exlcude_puncuation:        \n",
    "        # Stem the word\n",
    "            stem_word = ps.stem(word)\n",
    "#             print(stem_word)\n",
    "#             print(\"Term: \", stem_word)\n",
    "            print(stem_word, end=' ')\n",
    "            if stem_word in inv_index.keys():\n",
    "                processed = True\n",
    "                posting = inv_index[stem_word]\n",
    "                #print(\"Index: \", posting)\n",
    "                if stem_word not in term_freq_table.keys():\n",
    "                    term_freq_table[stem_word] = word_frequency_count(posting)[1]\n",
    "                    term_doc_score = query_doc_score(stem_word)\n",
    "#                     print(stem_word, end='+')\n",
    "                    update_document_score_query(term_doc_score)\n",
    "\n",
    "    #             print(stem_word,\" Occurred in \", count_word_frequency(posting)[0], \\\n",
    "    #                   \" documents\", count_word_frequency(posting)[1] ,\" times\\n\")\n",
    "            else:\n",
    "                not_found.append(stem_word)\n",
    "    if len(not_found)!=0:\n",
    "        print(\"\\nNo match found for : \", not_found)\n",
    "    return processed,not_found\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Query processor engine\n",
    "if TEST_QUERY:\n",
    "\n",
    "    query = \"ramanathan perumal\"\n",
    "    document_score_query.clear()\n",
    "\n",
    "    start_time = time.time() # measure time taken to search data   \n",
    "    print(\"Input Query ->\",query)\n",
    "    print(\"Processing Query :\", end=':')\n",
    "    result,not_found = query_processor_engine(query)\n",
    "    stop_time = time.time() # stop the timer\n",
    "    time_taken = stop_time-start_time\n",
    "    print(\"\\n\\nFound \",len(document_score_query), \"results\", \"in\",str(np.round(time_taken,4))+\" sec\" )\n",
    "    results = {}\n",
    "    if result == True:\n",
    "    #     print(document_score_query)\n",
    "        for i, key in zip(range(len(document_score_query)),sorted(document_score_query, key=document_score_query.get,reverse=True)):\n",
    "            print(\"\\nDoc_Id: \",key, \"Relevance Score: \",round(document_score_query[key],4))\n",
    "            print(\"Title: \",display_data.iloc[key-1]['Title'])\n",
    "            print(\"Title Link: \", display_data.iloc[key-1]['Title_Link'])\n",
    "            print(\"Profile Name: \",display_data.iloc[key-1]['Profile_Name'] )        \n",
    "            print(\"Co Authors: \",display_data.iloc[key-1]['Co_Authors'])\n",
    "            results[key] = [round(document_score_query[key],4),\\\n",
    "                            display_data.iloc[key-1]['Title'],\\\n",
    "                            display_data.iloc[key-1]['Title_Link'],\\\n",
    "                            display_data.iloc[key-1]['Profile_Name'],\\\n",
    "                            display_data.iloc[key-1]['Co_Authors'],\\\n",
    "                            display_data.iloc[key-1]['No_Of_Citations']]       \n",
    "            break\n",
    "    else:\n",
    "        print(\"No matching data found...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template,request\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load subject classification  pre-requisite\n",
    "def subject_classification_model(query):\n",
    "    #load the subject classification model\n",
    "    model_name = 'NB_Model.sav'\n",
    "    subject_classify_model = pickle.load(open(model_name, 'rb')) \n",
    "    #Load training data to create vectorspace\n",
    "    x_train = pickle.load(open(\"NB_train_data.npy\",'rb'))\n",
    "#     print(x_train.shape,type(x_train))\n",
    "    # Form tf-idf vector\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    x_train_vector = vectorizer.fit_transform(x_train)\n",
    "    \n",
    "    stop_words = stopwords.words('english')\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    #convert into tokens, remove stop words and stem the tokens\n",
    "    tokenizer = RegexpTokenizer('[A-Za-z]\\w+')\n",
    "    test_input = tokenizer.tokenize(str(query))\n",
    "    \n",
    "    test_input = [ps.stem(token) for token in test_input if token not in stop_words]      \n",
    "    test_input =  [' '.join(map(str,test_input))]\n",
    "    \n",
    "    print(\"Inside function:\", test_input)\n",
    "    test_input = np.array(test_input)\n",
    "    test_vector = vectorizer.transform(test_input)        \n",
    "\n",
    "    return subject_classify_model, test_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__main__\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [02/Aug/2021 13:10:03] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching subject classification\n",
      "Football\n",
      "Query :  Football\n",
      "Inside function: ['footbal']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Aug/2021 13:10:08] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.00044499 11.14145388  9.10197508  9.23776092  8.49394886  9.18993396\n",
      "  10.19600926  8.96773925 16.25050129  8.42023252]]\n",
      "Searching subject classification\n",
      "virus\n",
      "Query :  virus\n",
      "Inside function: ['viru']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Aug/2021 13:10:23] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.5787941   8.61525792  8.26934767  8.39271213  7.71694226  8.73301252\n",
      "   9.26330216  8.14739142 10.05313259 21.23010721]]\n",
      "Searching subject classification\n",
      "trade\n",
      "Query :  trade\n",
      "Inside function: ['trade']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [02/Aug/2021 13:11:06] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26.57446351  8.96423318  6.48364957  6.72107151 11.62860098  6.65960338\n",
      "  11.0581723   6.5176093   7.28497398  8.10762228]]\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def vertical_Engine_IR():\n",
    "    query = []\n",
    "    errors = []\n",
    "    results={}\n",
    "    text_boot = False\n",
    "    status = []\n",
    "    not_found = []\n",
    "    search = True\n",
    "    subject=[]\n",
    "    input_mapping = {0:\"business\",\n",
    "                       1:\"entertainment\",\n",
    "                       2:\"food\",\n",
    "                       3:\"graphics\",\n",
    "                       4:\"historical\",\n",
    "                       5:\"medical\",\n",
    "                       6:\"politics\",\n",
    "                       7:\"space\",\n",
    "                       8:\"sport\",\n",
    "                       9:\"technologie\",\n",
    "                    } \n",
    "    \n",
    "\n",
    "    #if user has entered the search query\n",
    "    if request.method == 'POST':\n",
    "        try:\n",
    "            #get user query\n",
    "            query = request.form['text_Name']\n",
    "            \n",
    "            if request.form['search_query']=='Searching CU Authors':\n",
    "                print(\"Searching CU Authors\")\n",
    "                search =True\n",
    "            elif request.form['search_query']=='Searching Subject Classification':\n",
    "                print(\"Searching subject classification\")\n",
    "                search=False\n",
    "            \n",
    "            document_score_query.clear()\n",
    "            results.clear()              \n",
    "            status.clear()\n",
    "            not_found.clear()\n",
    "            subject=[]    \n",
    "            print(query)\n",
    "            \n",
    "        except:\n",
    "            errors.append(\"Query Error.\")\n",
    "            return render_template('index.html',errors = errors)\n",
    "        \n",
    "        if query and search==True:\n",
    "            \n",
    "            text_boot = True\n",
    "            start_time = time.time() # measure time taken to search data \n",
    "            print(\"input query ::\",query)\n",
    "            search,not_found = query_processor_engine(str(query))\n",
    "            stop_time = time.time() # stop the timer\n",
    "            time_taken = stop_time-start_time\n",
    "#           print(\"\\n\\nFound \",len(document_score_query), \"results\", \"in\",str(np.round(time_taken,4))+\" sec\" )\n",
    "            status.append(len(document_score_query)) # no. of documents found\n",
    "            status.append(np.round(time_taken,4)) # time taken to process query\n",
    "#             print(stats)\n",
    "            if search == True:\n",
    "                for i, key in zip(range(len(document_score_query)),sorted(document_score_query, key=document_score_query.get,reverse=True)):\n",
    "                    results[key] = [round(document_score_query[key],4),\\\n",
    "                                    display_data.iloc[key-1]['Title'],\\\n",
    "                                    display_data.iloc[key-1]['Title_Link'],\\\n",
    "                                    display_data.iloc[key-1]['Profile_Name'],\\\n",
    "                                    display_data.iloc[key-1]['Co_Authors'],\\\n",
    "                                    display_data.iloc[key-1]['No_Of_Citations'] ]\n",
    "            else:\n",
    "                results = ''\n",
    "        else:\n",
    "            results = ''  # set null for search result\n",
    "            print(\"Query : \",query)\n",
    "            subject_classify_model,test_vector = subject_classification_model(query)\n",
    "            \n",
    "            result = subject_classify_model.predict(test_vector)\n",
    "            prob = subject_classify_model.predict_proba(test_vector)\n",
    "            print(prob*100)\n",
    "            subject.append(input_mapping[result[0]])\n",
    "            subject.append(round(prob[0][np.argmax(prob)]*100,2))\n",
    "    return render_template('index.html',errors = errors, results=results,\\\n",
    "                           text_boot=text_boot,status=status,not_found=not_found,\\\n",
    "                          query = query,subject=subject)\n",
    "\n",
    "print(__name__)\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True,use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
